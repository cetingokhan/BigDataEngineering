version: '3'

services:
  namenode:
    image: hadoop-namenode:latest
    container_name: namenode
    build:
      context: .
      dockerfile: hadoop/Dockerfile
    networks:
      hbase:
        aliases:
          - namenode
    volumes:
      - namenode:/hadoop/dfs/name
    environment:
      - CLUSTER_NAME=test
      - NODE_TYPE=name
    env_file:
      - ./hadoop.env
    ports:
      - "9870:9870"
    deploy:
      mode: replicated
      replicas: 1
      restart_policy:
        condition: on-failure
      placement:
        constraints:
          - node.hostname == practice.hadoop.io
      labels:
        traefik.docker.network: hbase
        traefik.port: 9870

  datanode1:
    image: hadoop-datanode:latest
    container_name: datanode1
    build:
      context: .
      dockerfile: hadoop/Dockerfile
    networks:
      hbase:
        aliases:
          - datanode1
    volumes:
      - datanode1:/hadoop/dfs/data
    env_file:
      - ./hadoop.env
    environment:
      - NODE_TYPE=data
      - SERVICE_PRECONDITION=namenode:9870
    deploy:
      mode: global
      restart_policy:
        condition: on-failure
      labels:
        traefik.docker.network: hbase
        traefik.port: 9864

  datanode2:
    image: hadoop-datanode:latest
    container_name: datanode2
    build:
      context: .
      dockerfile: hadoop/Dockerfile
    networks:
      hbase:
        aliases:
          - datanode2
    volumes:
      - datanode2:/hadoop/dfs/data
    env_file:
      - ./hadoop.env
    environment:
      - SERVICE_PRECONDITION=namenode:9870
      - NODE_TYPE=data
    deploy:
      mode: global
      restart_policy:
        condition: on-failure
      labels:
        traefik.docker.network: hbase
        traefik.port: 9865

  spark-master:
    image: bde2020/spark-master:3.3.0-hadoop3.3
    networks:
      - hbase
    container_name: spark-master
    ports:
      - "65080:8080"
      - "7077:7077"
    environment:
      - INIT_DAEMON_STEP=setup_spark
  spark-worker-1:
    image: bde2020/spark-worker:3.3.0-hadoop3.3
    networks:
      - hbase
    container_name: spark-worker-1
    depends_on:
      - spark-master
    ports:
      - "65081:8081"
    environment:
      - "SPARK_MASTER=spark://spark-master:7077"
  spark-worker-2:
    image: bde2020/spark-worker:3.3.0-hadoop3.3
    networks:
      - hbase
    container_name: spark-worker-2
    depends_on:
      - spark-master
    ports:
      - "65082:8081"
    environment:
      - "SPARK_MASTER=spark://spark-master:7077"
  spark-history-server:
      image: bde2020/spark-history-server:3.3.0-hadoop3.3
      networks:
        - hbase
      container_name: spark-history-server
      depends_on:
        - spark-master
      ports:
        - "18081:18081"
      volumes:
        - /tmp/spark-events-local:/tmp/spark-events

volumes:
  datanode1:
  datanode2:
  namenode:

networks:
  hbase:
    name: hbase